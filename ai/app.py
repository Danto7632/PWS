import streamlit as st
import ollama
import PyPDF2
import chromadb
from sentence_transformers import SentenceTransformer
import os
import tempfile
import pandas as pd
from typing import List, Dict, Optional
import warnings
import random
import time
warnings.filterwarnings("ignore")

# ì„ íƒì  LLM ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

try:
    import google.generativeai as genai
except ImportError:
    genai = None

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì‹¤ì „í˜• ì—…ë¬´ ì‹œë®¬ë ˆì´í„° for ì‹ ì…",
    page_icon="ğŸ¯",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ìŠ¤íƒ€ì¼ ì ìš©
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        text-align: center;
        margin-bottom: 2rem;
        background: linear-gradient(90deg, #ff6b6b, #4ecdc4);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
    }
    .role-badge {
        padding: 0.25rem 0.75rem;
        border-radius: 15px;
        font-weight: bold;
        margin-bottom: 0.5rem;
    }
    .customer-badge {
        background-color: #ff4757;
        color: white;
    }
    .employee-badge {
        background-color: #2ed573;
        color: white;
    }
    .simulation-box {
        border: 2px solid #e1e8ed;
        border-radius: 10px;
        padding: 1.5rem;
        margin: 1rem 0;
    }
    .stats-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin: 0.5rem 0;
    }
</style>
""", unsafe_allow_html=True)

# -----------------------------
# Chroma / Embedding ì´ˆê¸°í™”
# -----------------------------
@st.cache_resource
def init_chroma_client():
    return chromadb.PersistentClient(path="./work_simulator_db")

@st.cache_resource
def init_embedding_model():
    return SentenceTransformer('all-MiniLM-L6-v2')

# Ollama ì—°ê²° í™•ì¸
def check_ollama_connection():
    try:
        models = ollama.list()
        return True, models
    except Exception as e:
        return False, str(e)

# -----------------------------
# ë¬¸ì„œ ì²˜ë¦¬ í•¨ìˆ˜
# -----------------------------
def extract_text_from_pdf(pdf_file) -> str:
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            tmp_file.write(pdf_file.getvalue())
            tmp_file_path = tmp_file.name

        text = ""
        with open(tmp_file_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"

        os.unlink(tmp_file_path)
        return text
    except Exception as e:
        st.error(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
        return ""

def extract_text_from_txt(txt_file) -> str:
    try:
        content = txt_file.getvalue()
        if isinstance(content, bytes):
            content = content.decode('utf-8')
        return content
    except Exception as e:
        st.error(f"TXT íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}")
        return ""

def extract_text_from_excel(excel_file) -> str:
    try:
        df = pd.read_excel(excel_file)
        text = df.to_string(index=False)
        return text
    except Exception as e:
        st.error(f"Excel íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}")
        return ""

def process_uploaded_file(uploaded_file) -> str:
    """ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    file_type = uploaded_file.type

    if file_type == "application/pdf":
        return extract_text_from_pdf(uploaded_file)
    elif file_type == "text/plain":
        return extract_text_from_txt(uploaded_file)
    elif file_type in [
        "application/vnd.ms-excel",
        "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    ]:
        return extract_text_from_excel(uploaded_file)
    else:
        st.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_type}")
        return ""

def chunk_text(text: str, chunk_size: int = 800, overlap: int = 100) -> List[str]:
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        if chunk.strip():
            chunks.append(chunk)
        start = end - overlap
    return chunks

def create_knowledge_base(chunks: List[str], embedding_model, collection_name: str = "work_manual"):
    client = init_chroma_client()

    try:
        client.delete_collection(name=collection_name)
    except:
        pass

    collection = client.create_collection(name=collection_name)

    for i, chunk in enumerate(chunks):
        embedding = embedding_model.encode(chunk).tolist()
        collection.add(
            embeddings=[embedding],
            documents=[chunk],
            ids=[f"chunk_{i}"]
        )

    return collection

def search_knowledge_base(query: str, collection, embedding_model, top_k: int = 3) -> List[str]:
    query_embedding = embedding_model.encode(query).tolist()
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k
    )
    return results['documents'][0] if results['documents'] else []

# -----------------------------
# ê³µí†µ LLM í˜¸ì¶œ í•¨ìˆ˜
# -----------------------------
def call_llm(prompt: str) -> str:
    provider = st.session_state.get("llm_provider", "ollama")
    model_name = st.session_state.get("model_name", "exaone3.5:2.4b-jetson")

    # 1) ë¡œì»¬ Ollama
    if provider == "ollama":
        try:
            resp = ollama.chat(
                model=model_name,
                messages=[{"role": "user", "content": prompt}]
            )
            return resp["message"]["content"].strip()
        except Exception as e:
            st.error(f"ë¡œì»¬ Ollama í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return ""

    # 2) OpenAI GPT
    elif provider == "openai":
        api_key = st.session_state.get("openai_api_key", "")
        if not api_key:
            st.error("OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
            return ""
        if OpenAI is None:
            st.error("openai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. `pip install openai` í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
            return ""
        try:
            client = OpenAI(api_key=api_key)
            response = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            st.error(f"OpenAI í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return ""

    # 3) Google Gemini
    elif provider == "gemini":
        api_key = st.session_state.get("gemini_api_key", "")
        if not api_key:
            st.error("Gemini API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
            return ""
        if genai is None:
            st.error("google-generativeai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. `pip install google-generativeai` í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
            return ""
        try:
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(prompt)
            text = getattr(response, "text", "") or ""
            return text.strip()
        except Exception as e:
            st.error(f"Gemini í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return ""

    else:
        st.error("ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ê³µê¸‰ìì…ë‹ˆë‹¤.")
        return ""

# -----------------------------
# ì‹œë®¬ë ˆì´ì…˜ AI í•¨ìˆ˜ë“¤
# -----------------------------
def generate_customer_scenario(context: str) -> Dict[str, str]:
    """ì—…ë¡œë“œí•œ ë§¤ë‰´ì–¼ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê³ ê° ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±"""
    try:
        prompt = f"""
ë‹¹ì‹ ì€ ì•„ë˜ ë§¤ë‰´ì–¼ì— ë‚˜ì˜¤ëŠ” ì„œë¹„ìŠ¤/ì—…ë¬´ì˜ ê³ ê° ë˜ëŠ” ì‚¬ìš©ìì…ë‹ˆë‹¤.

[ì—…ë¬´/ì„œë¹„ìŠ¤ ë§¤ë‰´ì–¼ ë°œì·Œ]
{context[:1500]}

ìœ„ ë§¤ë‰´ì–¼ì˜ ì£¼ì œì™€ ìš©ì–´ë¥¼ ë²—ì–´ë‚˜ì§€ ë§ê³ ,
ì‹¤ì œ í˜„ì¥ì—ì„œ ìì£¼ ë‚˜ì˜¬ ë²•í•œ ê³ ê° ë¬¸ì˜ ìƒí™© 1ê°œë§Œ ë§Œë“œì„¸ìš”.

ë°˜ë“œì‹œ ë§¤ë‰´ì–¼ì˜ ë‚´ìš©ê³¼ ì§ì ‘ ê´€ë ¨ëœ ë¬¸ì˜ì—¬ì•¼ í•˜ë©°,
ë§¤ë‰´ì–¼ì— ì—†ëŠ” ìƒˆë¡œìš´ ì¢…ë¥˜ì˜ ìƒí’ˆ/ì„œë¹„ìŠ¤(ì˜·, ì¬í‚·, ìŒì‹, íƒë°°, í•­ê³µê¶Œ ë“±)ëŠ” ë§Œë“¤ì§€ ë§ˆì„¸ìš”.

[ì¶œë ¥ í˜•ì‹ - ì´ í˜•ì‹ ê·¸ëŒ€ë¡œ]
ìƒí™©: (ê³ ê°ì´ ì²˜í•œ ìƒí™©ì„ í•œ ì¤„ë¡œ)
ê³ ê° ìœ í˜•: (ì˜ˆ: ì¼ë°˜ ê³ ê° / ì´ˆë³´ í•™ìŠµì / ì»´í“¨í„°ì— ìµìˆ™í•˜ì§€ ì•Šì€ ê³ ê° ë“±)
ê³ ê° ì²« ë§: (ì§ì›ì—ê²Œ ì²˜ìŒ ê±´ë„¤ëŠ” í•œ ë¬¸ì¥)
""".strip()

        content = call_llm(prompt).strip()

        scenario = {
            'situation': '',
            'customer_type': '',
            'first_message': ''
        }

        for line in content.splitlines():
            line = line.strip()
            if line.startswith("ìƒí™©:"):
                scenario['situation'] = line.split("ìƒí™©:", 1)[1].strip()
            elif line.startswith("ê³ ê° ìœ í˜•:"):
                scenario['customer_type'] = line.split("ê³ ê° ìœ í˜•:", 1)[1].strip()
            elif line.startswith("ê³ ê° ì²« ë§:") or line.startswith("ì²« ë§:") or "ê³ ê° ì²« ë§:" in line:
                scenario['first_message'] = line.split(":", 1)[1].strip().strip('"â€œâ€')

        if not scenario['situation']:
            scenario['situation'] = "ë§¤ë‰´ì–¼ì— ë‚˜ì˜¨ ë‚´ìš©ì„ ë¬¸ì˜í•˜ê¸° ìœ„í•´ ì—°ë½í•œ ê³ ê°"
        if not scenario['customer_type']:
            scenario['customer_type'] = "ì¼ë°˜ ê³ ê°"
        if not scenario['first_message']:
            scenario['first_message'] = "ì•ˆë…•í•˜ì„¸ìš”, ë§¤ë‰´ì–¼ ë‚´ìš© ê´€ë ¨í•´ì„œ ëª‡ ê°€ì§€ ë¬¸ì˜ë“œë¦¬ê³  ì‹¶ìŠµë‹ˆë‹¤."

        return scenario

    except Exception:
        return {
            'situation': 'ì¼ë°˜ì ì¸ ë¬¸ì˜ ìƒí™©',
            'customer_type': 'ì¼ë°˜ ê³ ê°',
            'first_message': 'ì•ˆë…•í•˜ì„¸ìš”, ë¬¸ì˜ì‚¬í•­ì´ ìˆì–´ì„œ ì—°ë½ë“œë ¸ìŠµë‹ˆë‹¤.'
        }

def customer_ai_response(user_message: str, context: str, scenario: Dict) -> str:
    """ê³ ê° AI ì‘ë‹µ ìƒì„± (ë§¤ë‰´ì–¼ ê¸°ë°˜)"""
    try:
        prompt = f"""ë‹¹ì‹ ì€ ë‹¤ìŒ ìƒí™©ì˜ ê³ ê°ì…ë‹ˆë‹¤.

[ì—…ë¬´/ì„œë¹„ìŠ¤ ë§¤ë‰´ì–¼ ë°œì·Œ]
{context[:800]}

ìƒí™©: {scenario.get('situation', '')}
ê³ ê° ìœ í˜•: {scenario.get('customer_type', '')}

ìœ„ ë§¤ë‰´ì–¼ì˜ ì£¼ì œì™€ ìš©ì–´ë¥¼ ë²—ì–´ë‚˜ì§€ ë§ê³ ,
ì§ì›ì˜ ë‹µë³€ì„ ë“¤ì€ ë’¤ ì´ì–´ì§ˆ ë‹¤ìŒ ê³ ê° ì§ˆë¬¸/ë°˜ì‘ì„ í•œ ë¬¸ì¥ìœ¼ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”.
ë§¤ë‰´ì–¼ì— ì—†ëŠ” ìƒˆë¡œìš´ ì¢…ë¥˜ì˜ ìƒí’ˆ/ì„œë¹„ìŠ¤(ì˜·, ì¬í‚·, ìŒì‹, íƒë°° ë“±)ëŠ” ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.

ì§ì› ì‘ë‹µ: {user_message}

ê³ ê° ë‹µë³€ (50ì ì´ë‚´, í•œ ë¬¸ì¥):"""

        return call_llm(prompt).strip()
    except Exception:
        return "ë„¤, ì•Œê² ìŠµë‹ˆë‹¤. ì•ˆë‚´í•´ ì£¼ì‹  ë‚´ìš©ìœ¼ë¡œ ì§„í–‰í•´ ë³¼ê²Œìš”."

def employee_ai_response(user_message: str, context: str) -> str:
    """ì§ì› AI ì‘ë‹µ ìƒì„±"""
    try:
        prompt = f"""ë‹¤ìŒ ì—…ë¬´ ë§¤ë‰´ì–¼ì„ ì°¸ê³ í•˜ì—¬ ê³ ê° ë¬¸ì˜ì— ì „ë¬¸ì ì´ê³  ì¹œì ˆí•˜ê²Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

ì—…ë¬´ ë§¤ë‰´ì–¼:
{context}

ê³ ê° ë¬¸ì˜: {user_message}

ì¹œì ˆí•˜ê³  ì •í™•í•œ ì§ì› ì‘ë‹µ (100ì ì´ë‚´):"""

        return call_llm(prompt).strip()
    except Exception:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. í™•ì¸ í›„ ë‹¤ì‹œ ì•ˆë‚´í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."

def evaluate_response(user_response: str, context: str) -> Dict[str, any]:
    """ì‚¬ìš©ì ì‘ë‹µ í‰ê°€"""
    try:
        prompt = f"""ë‹¤ìŒ ì—…ë¬´ ë§¤ë‰´ì–¼ì„ ê¸°ì¤€ìœ¼ë¡œ ì§ì›ì˜ ê³ ê° ì‘ë‹µì„ í‰ê°€í•´ì£¼ì„¸ìš”:

ì—…ë¬´ ë§¤ë‰´ì–¼:
{context[:1000]}

ì§ì› ì‘ë‹µ: {user_response}

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”:
1. ì •í™•ì„± (1-5ì )
2. ì¹œì ˆì„± (1-5ì )  
3. ì ì ˆì„± (1-5ì )
ì´ì : /15ì 

í˜•ì‹:
ì •í™•ì„±: X/5 - ê°„ë‹¨í•œ ì½”ë©˜íŠ¸
ì¹œì ˆì„±: X/5 - ê°„ë‹¨í•œ ì½”ë©˜íŠ¸  
ì ì ˆì„±: X/5 - ê°„ë‹¨í•œ ì½”ë©˜íŠ¸
ì´ì : X/15
ê°œì„ ì : êµ¬ì²´ì ì¸ ê°œì„  ì œì•ˆ"""

        content = call_llm(prompt)

        total_score = 12
        try:
            if 'ì´ì :' in content:
                score_line = [line for line in content.split('\n') if 'ì´ì :' in line][0]
                total_score = int(score_line.split('/')[0].split(':')[-1].strip())
        except:
            pass

        return {
            'score': total_score,
            'max_score': 15,
            'feedback': content
        }
    except Exception:
        return {
            'score': 10,
            'max_score': 15,
            'feedback': 'í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'
        }

# -----------------------------
# ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
# -----------------------------
def main():
    # í—¤ë”
    st.markdown('<div class="main-header">ğŸ¯ ì‹¤ì „í˜• ì—…ë¬´ ì‹œë®¬ë ˆì´í„° for ì‹ ì…</div>', unsafe_allow_html=True)
    st.markdown("### ğŸ’¼ ì‹ ì… ì§ì›ì„ ìœ„í•œ ê³ ê° ì‘ëŒ€ ì—°ìŠµ ë„êµ¬")

    # ì‚¬ì´ë“œë°” - ì„¤ì • ë° ë¬¸ì„œ ì—…ë¡œë“œ
    with st.sidebar:
        st.header("ğŸ“š ì—…ë¬´ ë§¤ë‰´ì–¼ ì—…ë¡œë“œ")
        uploaded_files = st.file_uploader(
            "ë§¤ë‰´ì–¼ íŒŒì¼ë“¤ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
            type=['pdf', 'txt', 'xlsx', 'xls'],
            accept_multiple_files=True,
            help="PDF, TXT, Excel íŒŒì¼ì„ ì§€ì›í•©ë‹ˆë‹¤"
        )

        # ì„ë² ë”© í•™ìŠµ ìˆ˜ì¤€ ì„¤ì •
        st.header("ğŸ§  ì„ë² ë”© ì„¤ì •")
        embed_percent = st.slider(
            "íŒŒì¼ ì„ë² ë”© í•™ìŠµ ìˆ˜ì¤€ (%)",
            min_value=20,
            max_value=100,
            value=100,
            step=20,
            help="ë§¤ë‰´ì–¼ ì „ì²´ í…ìŠ¤íŠ¸ ì¤‘ ì„ë² ë”©ì— ì‚¬ìš©í•  ë¹„ìœ¨ì…ë‹ˆë‹¤."
        )
        st.session_state["embed_ratio"] = embed_percent / 100.0

        # LLM ì„¤ì •
        st.header("âš™ï¸ AI ì„¤ì •")

        llm_provider = st.selectbox(
            "LLM ê³µê¸‰ì",
            options=["ollama", "openai", "gemini"],
            format_func=lambda v: {
                "ollama": "ë¡œì»¬(Ollama)",
                "openai": "OpenAI GPT",
                "gemini": "Google Gemini"
            }[v]
        )
        st.session_state["llm_provider"] = llm_provider

        model_name = None

        if llm_provider == "ollama":
            if 'ollama_connected' not in st.session_state:
                with st.spinner("ğŸ” ë¡œì»¬ Ollama ì—°ê²° í™•ì¸ ì¤‘..."):
                    connected, result = check_ollama_connection()
                    st.session_state['ollama_connected'] = connected

            if st.session_state.get('ollama_connected', False):
                st.success("âœ… ë¡œì»¬ Ollama ì—°ê²°ë¨")
            else:
                st.warning("âš ï¸ Ollama ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© ì‹œ Ollama ì„œë²„ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

            model_name = st.selectbox(
                "Ollama ëª¨ë¸ ì„ íƒ",
                ["exaone3.5:2.4b-jetson", "llama3.2", "gemma2"],
                index=0
            )

        elif llm_provider == "openai":
            openai_key = st.text_input("OpenAI API Key", type="password")
            st.session_state["openai_api_key"] = openai_key

            openai_models = [
                "gpt-4.1-mini",
                "gpt-4.1",
                "gpt-4o-mini",
                "gpt-4o",
            ]
            model_name = st.selectbox(
                "OpenAI GPT ëª¨ë¸ ì„ íƒ",
                options=openai_models,
                index=0
            )

        elif llm_provider == "gemini":
            gemini_key = st.text_input("Gemini API Key", type="password")
            st.session_state["gemini_api_key"] = gemini_key

            gemini_models = [
                "gemini-2.5-flash",
                "gemini-2.5-pro",
                "gemini-1.5-flash",
                "gemini-1.5-pro",
            ]
            model_name = st.selectbox(
                "Gemini ëª¨ë¸ ì„ íƒ",
                options=gemini_models,
                index=0
            )

        if model_name:
            st.session_state["model_name"] = model_name

        # í•™ìŠµ í†µê³„
        st.header("ğŸ“Š í•™ìŠµ í†µê³„")
        if 'stats' not in st.session_state:
            st.session_state.stats = {
                'total_simulations': 0,
                'customer_role_count': 0,
                'employee_role_count': 0,
                'avg_score': 0.0,
                'total_score': 0
            }
        stats = st.session_state.stats

        st.markdown(f"""
        <div class="stats-card">
            <h4>ì´ ì‹œë®¬ë ˆì´ì…˜: {stats['total_simulations']}</h4>
        </div>
        <div class="stats-card">
            <h4>ê³ ê° ì—­í• : {stats['customer_role_count']}</h4>
        </div>
        <div class="stats-card">
            <h4>ì§ì› ì—­í• : {stats['employee_role_count']}</h4>
        </div>
        """, unsafe_allow_html=True)

    # ë©”ì¸ ì»¨í…ì¸ 
    if uploaded_files:
        # ë¬¸ì„œ ì²˜ë¦¬
        if st.button("ğŸ“– ë§¤ë‰´ì–¼ í•™ìŠµ ì‹œì‘", type="primary"):
            with st.spinner("ğŸ“š ì—…ë¬´ ë§¤ë‰´ì–¼ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                all_text = ""
                for file in uploaded_files:
                    text = process_uploaded_file(file)
                    all_text += f"\n\n=== {file.name} ===\n{text}"

                if all_text:
                    st.success(f"âœ… {len(uploaded_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")

                    embedding_model = init_embedding_model()
                    chunks = chunk_text(all_text)

                    ratio = st.session_state.get("embed_ratio", 1.0)
                    use_n = max(1, int(len(chunks) * ratio))
                    chunks_to_use = chunks[:use_n]

                    collection = create_knowledge_base(chunks_to_use, embedding_model)

                    st.session_state['knowledge_base'] = collection
                    st.session_state['embedding_model'] = embedding_model
                    st.session_state['manual_content'] = all_text

                    st.info(
                        f"ğŸ“– ì´ {len(chunks)}ê°œ ì²­í¬ ì¤‘ {use_n}ê°œë¥¼ ì„ë² ë”©í–ˆìŠµë‹ˆë‹¤. "
                        f"(í•™ìŠµ ìˆ˜ì¤€ {int(ratio * 100)}%)"
                    )

        # ì‹œë®¬ë ˆì´ì…˜ ì„¹ì…˜
        if 'knowledge_base' in st.session_state:
            st.markdown("---")

            col1, col2 = st.columns(2)

            with col1:
                st.markdown("""
                <div class="simulation-box">
                    <h3>ğŸ‘¤ ê³ ê° ì—­í•  ì—°ìŠµ</h3>
                    <p>AIê°€ ì§ì›ì´ ë˜ì–´ ê³ ê°ì¸ ë‹¹ì‹ ì˜ ë¬¸ì˜ì— ì‘ë‹µí•©ë‹ˆë‹¤.</p>
                </div>
                """, unsafe_allow_html=True)

                if st.button("ê³ ê°ìœ¼ë¡œ ì—°ìŠµí•˜ê¸°", key="customer_practice"):
                    st.session_state.current_role = "customer"
                    st.session_state.simulation_active = True
                    st.session_state.conversation_history = []
                    st.session_state.stats['customer_role_count'] += 1
                    st.rerun()

            with col2:
                st.markdown("""
                <div class="simulation-box">
                    <h3>ğŸ‘” ì§ì› ì—­í•  ì—°ìŠµ</h3>
                    <p>AIê°€ ë‹¤ì–‘í•œ ê³ ê°ì´ ë˜ì–´ ë‹¹ì‹ ì´ ì‘ëŒ€í•´ì•¼ í•  ìƒí™©ì„ ë§Œë“­ë‹ˆë‹¤.</p>
                </div>
                """, unsafe_allow_html=True)

                if st.button("ì§ì›ìœ¼ë¡œ ì—°ìŠµí•˜ê¸°", key="employee_practice"):
                    st.session_state.current_role = "employee"
                    st.session_state.simulation_active = True
                    st.session_state.conversation_history = []
                    st.session_state.stats['employee_role_count'] += 1

                    manual_text = st.session_state.get('manual_content', '')
                    scenario = generate_customer_scenario(manual_text)
                    st.session_state.customer_scenario = scenario

                    st.rerun()

            # ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
            if hasattr(st.session_state, 'simulation_active') and st.session_state.simulation_active:
                st.markdown("---")

                if st.session_state.current_role == "customer":
                    st.markdown('<div class="role-badge customer-badge">ğŸ‘¤ ë‹¹ì‹ ì˜ ì—­í• : ê³ ê°</div>', unsafe_allow_html=True)
                    st.markdown("**ğŸ’¡ ìƒí™©:** AI ì§ì›ì—ê²Œ ë¬¸ì˜ì‚¬í•­ì„ ë§í•´ë³´ì„¸ìš”.")
                else:
                    st.markdown('<div class="role-badge employee-badge">ğŸ‘” ë‹¹ì‹ ì˜ ì—­í• : ì§ì›</div>', unsafe_allow_html=True)
                    scenario = st.session_state.get('customer_scenario', {})
                    if scenario:
                        st.markdown(f"""
                        **ğŸ“‹ ìƒí™©:** {scenario.get('situation', '')}  
                        **ğŸ‘¥ ê³ ê° ìœ í˜•:** {scenario.get('customer_type', '')}  
                        **ğŸ’¬ ê³ ê° ì²« ë§:** "{scenario.get('first_message', '')}"
                        """)

                if 'conversation_history' not in st.session_state:
                    st.session_state.conversation_history = []

                # ì§ì› ëª¨ë“œ ì²« í„´: ê³ ê° ì²« ë°œí™” ìë™ ì¶”ê°€
                if (st.session_state.current_role == "employee"
                    and not st.session_state.conversation_history
                    and 'customer_scenario' in st.session_state):
                    first_msg = st.session_state.customer_scenario.get('first_message', '')
                    if first_msg:
                        st.session_state.conversation_history.append({
                            'role': 'customer_ai',
                            'message': first_msg
                        })

                # ëŒ€í™” í‘œì‹œ
                for msg in st.session_state.conversation_history:
                    if msg['role'] == 'user':
                        if st.session_state.current_role == "customer":
                            with st.chat_message("user"):
                                st.markdown(f"**ê³ ê° (ë‹¹ì‹ ):** {msg['message']}")
                        else:
                            with st.chat_message("user"):
                                st.markdown(f"**ì§ì› (ë‹¹ì‹ ):** {msg['message']}")
                    elif msg['role'] == 'employee_ai':
                        with st.chat_message("assistant"):
                            st.markdown(f"**AI ì§ì›:** {msg['message']}")
                    elif msg['role'] == 'customer_ai':
                        with st.chat_message("assistant"):
                            st.markdown(f"**AI ê³ ê°:** {msg['message']}")

                # ì…ë ¥ì°½
                if st.session_state.current_role == "customer":
                    user_input = st.chat_input("ê³ ê°ìœ¼ë¡œì„œ ë¬¸ì˜ì‚¬í•­ì„ ì…ë ¥í•˜ì„¸ìš”...")
                else:
                    user_input = st.chat_input("ì§ì›ìœ¼ë¡œì„œ ì‘ë‹µì„ ì…ë ¥í•˜ì„¸ìš”...")

                if user_input:
                    st.session_state.conversation_history.append({
                        'role': 'user',
                        'message': user_input
                    })

                    context = search_knowledge_base(
                        user_input,
                        st.session_state['knowledge_base'],
                        st.session_state['embedding_model']
                    )
                    context_text = " ".join(context)

                    if st.session_state.current_role == "customer":
                        ai_response = employee_ai_response(user_input, context_text)
                        st.session_state.conversation_history.append({
                            'role': 'employee_ai',
                            'message': ai_response
                        })
                    else:
                        # ì§ì› ëª¨ë“œ: ë‹µë³€ â†’ í‰ê°€ â†’ ìƒˆ ì§ˆë¬¸
                        evaluation = evaluate_response(user_input, context_text)
                        st.session_state.last_evaluation = evaluation

                        stats = st.session_state.stats
                        stats['total_score'] += evaluation['score']
                        stats['total_simulations'] += 1
                        if stats['total_simulations'] > 0:
                            stats['avg_score'] = stats['total_score'] / stats['total_simulations']

                        manual_text = st.session_state.get('manual_content', '')
                        next_scenario = generate_customer_scenario(manual_text)
                        st.session_state.customer_scenario = next_scenario

                        next_first = next_scenario.get('first_message', '')
                        if next_first:
                            st.session_state.conversation_history.append({
                                'role': 'customer_ai',
                                'message': next_first
                            })

                    st.rerun()

                # í‰ê°€ ê²°ê³¼ (ì§ì› ëª¨ë“œë§Œ)
                if (st.session_state.current_role == "employee"
                    and hasattr(st.session_state, 'last_evaluation')):
                    eval_data = st.session_state.last_evaluation
                    st.markdown("### ğŸ“Š ì‘ë‹µ í‰ê°€")
                    col_eval1, col_eval2 = st.columns([1, 2])
                    with col_eval1:
                        score_percentage = (eval_data['score'] / eval_data['max_score']) * 100
                        st.metric("ì ìˆ˜", f"{eval_data['score']}/{eval_data['max_score']}", f"{score_percentage:.0f}%")
                    with col_eval2:
                        st.text_area("ìƒì„¸ í”¼ë“œë°±", eval_data['feedback'], height=100, disabled=True)

                # ì¢…ë£Œ / ìƒˆ ì‹œë‚˜ë¦¬ì˜¤
                col_end1, col_end2 = st.columns([1, 1])
                with col_end1:
                    if st.button("ğŸ”„ ìƒˆ ì‹œë‚˜ë¦¬ì˜¤ ì‹œì‘"):
                        st.session_state.conversation_history = []
                        if st.session_state.current_role == "employee":
                            manual_text = st.session_state.get('manual_content', '')
                            scenario = generate_customer_scenario(manual_text)
                            st.session_state.customer_scenario = scenario
                        st.rerun()
                with col_end2:
                    if st.button("âŒ ì‹œë®¬ë ˆì´ì…˜ ì¢…ë£Œ"):
                        st.session_state.simulation_active = False
                        st.rerun()

    else:
        # ì†Œê°œ ì„¹ì…˜
        st.markdown("""
        ## ğŸš€ ì‹œì‘í•˜ê¸°
        
        ### 1ë‹¨ê³„: ì—…ë¬´ ë§¤ë‰´ì–¼ ì—…ë¡œë“œ
        - ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ì—…ë¬´ ë§¤ë‰´ì–¼ íŒŒì¼ë“¤ì„ ì—…ë¡œë“œí•˜ì„¸ìš”
        - **ì§€ì› í˜•ì‹:** PDF, TXT, Excel íŒŒì¼
        - **ì˜ˆì‹œ:** ê³ ê°ì‘ëŒ€ ë§¤ë‰´ì–¼, FAQ, ì„œë¹„ìŠ¤ ì•ˆë‚´ì„œ ë“±
        
        ### 2ë‹¨ê³„: ì—­í•  ì„ íƒ
        - **ğŸ‘¤ ê³ ê° ì—­í• :** AI ì§ì›ê³¼ ëŒ€í™”í•˜ë©° ê³ ê° ì…ì¥ ì²´í—˜
        - **ğŸ‘” ì§ì› ì—­í• :** AI ê³ ê°ì˜ ë¬¸ì˜ì— ì‘ëŒ€í•˜ë©° ì‹¤ì „ ì—°ìŠµ
        
        ### 3ë‹¨ê³„: ì‹¤ì „ ì—°ìŠµ
        - ì‹¤ì œ ì—…ë¬´ì™€ ìœ ì‚¬í•œ ìƒí™©ì—ì„œ ì—°ìŠµ
        - AIì˜ ì‹¤ì‹œê°„ í”¼ë“œë°±ìœ¼ë¡œ ê°œì„ ì  íŒŒì•…
        - ë°˜ë³µ í•™ìŠµìœ¼ë¡œ ìì‹ ê° í–¥ìƒ
        
        ---
        
        ## ğŸ’¡ ì£¼ìš” ê¸°ëŠ¥
        
        - âœ… **ë‹¤ì–‘í•œ ë¬¸ì„œ ì§€ì›**: PDF, TXT, Excel íŒŒì¼ ì—…ë¡œë“œ
        - âœ… **ì–‘ë°©í–¥ ì‹œë®¬ë ˆì´ì…˜**: ê³ ê°â†”ì§ì› ì—­í•  ì „í™˜
        - âœ… **ì‹¤ì‹œê°„ í‰ê°€**: AIì˜ ìƒì„¸í•œ í”¼ë“œë°± ì œê³µ
        - âœ… **í•™ìŠµ í†µê³„**: ì—°ìŠµ ì§„í–‰ë„ ë° ì„±ê³¼ ì¶”ì 
        - âœ… **ì‹¤ì „ ì‹œë‚˜ë¦¬ì˜¤**: ë§¤ë‰´ì–¼ ê¸°ë°˜ ë‹¤ì–‘í•œ ìƒí™© ìƒì„±
        
        **ğŸ¯ ì§€ê¸ˆ ë°”ë¡œ ë§¤ë‰´ì–¼ì„ ì—…ë¡œë“œí•˜ê³  ì—°ìŠµì„ ì‹œì‘í•´ë³´ì„¸ìš”!**
        """)

if __name__ == "__main__":
    main()
