import streamlit as st
import ollama
import PyPDF2
import chromadb
from sentence_transformers import SentenceTransformer
import os
import tempfile
import pandas as pd
from typing import List, Dict, Optional
import warnings
import random
import time
warnings.filterwarnings("ignore")

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì‹¤ì „í˜• ì—…ë¬´ ì‹œë®¬ë ˆì´í„° for ì‹ ì…",
    page_icon="ğŸ¯",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ìŠ¤íƒ€ì¼ ì ìš©
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        text-align: center;
        margin-bottom: 2rem;
        background: linear-gradient(90deg, #ff6b6b, #4ecdc4);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
    }
    .role-badge {
        padding: 0.25rem 0.75rem;
        border-radius: 15px;
        font-weight: bold;
        margin-bottom: 0.5rem;
    }
    .customer-badge {
        background-color: #ff4757;
        color: white;
    }
    .employee-badge {
        background-color: #2ed573;
        color: white;
    }
    .simulation-box {
        border: 2px solid #e1e8ed;
        border-radius: 10px;
        padding: 1.5rem;
        margin: 1rem 0;
    }
    .stats-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin: 0.5rem 0;
    }
</style>
""", unsafe_allow_html=True)

# ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
@st.cache_resource
def init_chroma_client():
    return chromadb.PersistentClient(path="./work_simulator_db")

# Sentence Transformer ëª¨ë¸ ì´ˆê¸°í™”
@st.cache_resource
def init_embedding_model():
    return SentenceTransformer('all-MiniLM-L6-v2')

# Ollama ì—°ê²° í™•ì¸
def check_ollama_connection():
    try:
        models = ollama.list()
        return True, models
    except Exception as e:
        return False, str(e)

# ë¬¸ì„œ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
def extract_text_from_pdf(pdf_file) -> str:
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            tmp_file.write(pdf_file.getvalue())
            tmp_file_path = tmp_file.name
        
        text = ""
        with open(tmp_file_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
        
        os.unlink(tmp_file_path)
        return text
    except Exception as e:
        st.error(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
        return ""

def extract_text_from_txt(txt_file) -> str:
    try:
        content = txt_file.getvalue()
        if isinstance(content, bytes):
            content = content.decode('utf-8')
        return content
    except Exception as e:
        st.error(f"TXT íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}")
        return ""

def extract_text_from_excel(excel_file) -> str:
    try:
        df = pd.read_excel(excel_file)
        # DataFrameì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        text = df.to_string(index=False)
        return text
    except Exception as e:
        st.error(f"Excel íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}")
        return ""

def process_uploaded_file(uploaded_file) -> str:
    """ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    file_type = uploaded_file.type
    
    if file_type == "application/pdf":
        return extract_text_from_pdf(uploaded_file)
    elif file_type == "text/plain":
        return extract_text_from_txt(uploaded_file)
    elif file_type in ["application/vnd.ms-excel", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"]:
        return extract_text_from_excel(uploaded_file)
    else:
        st.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_type}")
        return ""

def chunk_text(text: str, chunk_size: int = 800, overlap: int = 100) -> List[str]:
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        if chunk.strip():  # ë¹ˆ ì²­í¬ ì œì™¸
            chunks.append(chunk)
        start = end - overlap
    return chunks

def create_knowledge_base(chunks: List[str], embedding_model, collection_name: str = "work_manual"):
    client = init_chroma_client()
    
    try:
        client.delete_collection(name=collection_name)
    except:
        pass
    
    collection = client.create_collection(name=collection_name)
    
    for i, chunk in enumerate(chunks):
        embedding = embedding_model.encode(chunk).tolist()
        collection.add(
            embeddings=[embedding],
            documents=[chunk],
            ids=[f"chunk_{i}"]
        )
    
    return collection

def search_knowledge_base(query: str, collection, embedding_model, top_k: int = 3) -> List[str]:
    query_embedding = embedding_model.encode(query).tolist()
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k
    )
    return results['documents'][0] if results['documents'] else []

# ì‹œë®¬ë ˆì´ì…˜ AI í•¨ìˆ˜ë“¤
def generate_customer_scenario(context: str, model_name: str) -> Dict[str, str]:
    """ê³ ê° ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± (íŒŒì‹± ê°•í™” ë²„ì „)"""
    try:
        prompt = f"""
ë‹¹ì‹ ì€ ì½œì„¼í„°/ë§¤ì¥ ê³ ê°ì…ë‹ˆë‹¤. ì•„ë˜ ì—…ë¬´ ë§¤ë‰´ì–¼ì„ ì°¸ê³ í•´ì„œ,
ì‹¤ì œ ì—…ë¬´ì—ì„œ ìì£¼ ë‚˜ì˜¬ ë²•í•œ ê³ ê° ìƒí™© 1ê°€ì§€ë§Œ ë§Œë“œì„¸ìš”.

[ì¶œë ¥ í˜•ì‹ - ì´ í˜•ì‹ ê·¸ëŒ€ë¡œ, ë‹¤ë¥¸ ë¬¸ì¥ ì“°ì§€ ë§ ê²ƒ]

ìƒí™©: (ê³ ê°ì´ ì²˜í•œ ìƒí™©ì„ í•œ ì¤„ë¡œ)
ê³ ê° ìœ í˜•: (ì˜ˆ: ì¼ë°˜ ê³ ê° / ê¸‰í•œ ê³ ê° / ê¹Œë‹¤ë¡œìš´ ê³ ê° ë“±)
ê³ ê° ì²« ë§: (ì§ì›ì—ê²Œ ì²˜ìŒ ê±´ë„¤ëŠ” í•œ ë¬¸ì¥)

ì—…ë¬´ ë§¤ë‰´ì–¼:
{context[:1500]}
""".strip()

        response = ollama.chat(
            model=model_name,
            messages=[{'role': 'user', 'content': prompt}]
        )
        content = response['message']['content'].strip()

        scenario = {
            'situation': '',
            'customer_type': '',
            'first_message': ''
        }

        for line in content.splitlines():
            line = line.strip()
            if line.startswith("ìƒí™©:"):
                scenario['situation'] = line.split("ìƒí™©:", 1)[1].strip()
            elif line.startswith("ê³ ê° ìœ í˜•:"):
                scenario['customer_type'] = line.split("ê³ ê° ìœ í˜•:", 1)[1].strip()
            elif line.startswith("ê³ ê° ì²« ë§:") or line.startswith("ì²« ë§:") or "ì²« ë§:" in line:
                scenario['first_message'] = line.split(":", 1)[1].strip().strip('"â€œâ€')

        # LLMì´ ë§ì„ ì•ˆ ë“£ë”ë¼ë„ ê¸°ë³¸ê°’ ì±„ìš°ê¸°
        if not scenario['situation']:
            scenario['situation'] = "ìƒí’ˆê³¼ ì„œë¹„ìŠ¤ì— ëŒ€í•´ ë¬¸ì˜í•˜ê¸° ìœ„í•´ ì „í™”ë¥¼ ê±´ ê³ ê°"
        if not scenario['customer_type']:
            scenario['customer_type'] = "ì¼ë°˜ ê³ ê°"
        if not scenario['first_message']:
            scenario['first_message'] = "ì•ˆë…•í•˜ì„¸ìš”, ìƒí’ˆ ê´€ë ¨í•´ì„œ ëª‡ ê°€ì§€ ë¬¸ì˜ë“œë¦¬ê³  ì‹¶ìŠµë‹ˆë‹¤."

        return scenario

    except Exception:
        return {
            'situation': 'ì¼ë°˜ì ì¸ ë¬¸ì˜ ìƒí™©',
            'customer_type': 'ì¼ë°˜ ê³ ê°',
            'first_message': 'ì•ˆë…•í•˜ì„¸ìš”, ë¬¸ì˜ì‚¬í•­ì´ ìˆì–´ì„œ ì—°ë½ë“œë ¸ìŠµë‹ˆë‹¤.'
        }


def customer_ai_response(user_message: str, context: str, scenario: Dict, model_name: str) -> str:
    """ê³ ê° AI ì‘ë‹µ ìƒì„±"""
    try:
        prompt = f"""ë‹¹ì‹ ì€ ë‹¤ìŒ ìƒí™©ì˜ ê³ ê°ì…ë‹ˆë‹¤:

ìƒí™©: {scenario.get('situation', '')}
ê³ ê° ìœ í˜•: {scenario.get('customer_type', '')}

í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™”ì™€ ì§ì›ì˜ ì‘ë‹µì„ ë³´ê³ , ê³ ê°ìœ¼ë¡œì„œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€ë‹µí•´ì£¼ì„¸ìš”.
ì§ì› ì‘ë‹µ: {user_message}

ê³ ê°ë‹µë³€ (50ì ì´ë‚´ë¡œ ê°„ë‹¨íˆ):"""

        response = ollama.chat(
            model=model_name,
            messages=[{'role': 'user', 'content': prompt}]
        )
        
        return response['message']['content'].strip()
    except Exception as e:
        return "ë„¤, ì•Œê² ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤."

def employee_ai_response(user_message: str, context: str, model_name: str) -> str:
    """ì§ì› AI ì‘ë‹µ ìƒì„±"""
    try:
        prompt = f"""ë‹¤ìŒ ì—…ë¬´ ë§¤ë‰´ì–¼ì„ ì°¸ê³ í•˜ì—¬ ê³ ê° ë¬¸ì˜ì— ì „ë¬¸ì ì´ê³  ì¹œì ˆí•˜ê²Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

ì—…ë¬´ ë§¤ë‰´ì–¼:
{context}

ê³ ê° ë¬¸ì˜: {user_message}

ì¹œì ˆí•˜ê³  ì •í™•í•œ ì§ì› ì‘ë‹µ (100ì ì´ë‚´):"""

        response = ollama.chat(
            model=model_name,
            messages=[{'role': 'user', 'content': prompt}]
        )
        
        return response['message']['content'].strip()
    except Exception as e:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. í™•ì¸ í›„ ë‹¤ì‹œ ì•ˆë‚´í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."

def evaluate_response(user_response: str, context: str, model_name: str) -> Dict[str, any]:
    """ì‚¬ìš©ì ì‘ë‹µ í‰ê°€"""
    try:
        prompt = f"""ë‹¤ìŒ ì—…ë¬´ ë§¤ë‰´ì–¼ì„ ê¸°ì¤€ìœ¼ë¡œ ì§ì›ì˜ ê³ ê° ì‘ë‹µì„ í‰ê°€í•´ì£¼ì„¸ìš”:

ì—…ë¬´ ë§¤ë‰´ì–¼:
{context[:1000]}

ì§ì› ì‘ë‹µ: {user_response}

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”:
1. ì •í™•ì„± (1-5ì )
2. ì¹œì ˆì„± (1-5ì )  
3. ì ì ˆì„± (1-5ì )
ì´ì : /15ì 

í˜•ì‹:
ì •í™•ì„±: X/5 - ê°„ë‹¨í•œ ì½”ë©˜íŠ¸
ì¹œì ˆì„±: X/5 - ê°„ë‹¨í•œ ì½”ë©˜íŠ¸  
ì ì ˆì„±: X/5 - ê°„ë‹¨í•œ ì½”ë©˜íŠ¸
ì´ì : X/15
ê°œì„ ì : êµ¬ì²´ì ì¸ ê°œì„  ì œì•ˆ"""

        response = ollama.chat(
            model=model_name,
            messages=[{'role': 'user', 'content': prompt}]
        )
        
        content = response['message']['content']
        
        # ì ìˆ˜ ì¶”ì¶œ
        total_score = 12  # ê¸°ë³¸ ì ìˆ˜
        try:
            if 'ì´ì :' in content:
                score_line = [line for line in content.split('\n') if 'ì´ì :' in line][0]
                total_score = int(score_line.split('/')[0].split(':')[-1].strip())
        except:
            pass
        
        return {
            'score': total_score,
            'max_score': 15,
            'feedback': content
        }
    except Exception as e:
        return {
            'score': 10,
            'max_score': 15,
            'feedback': 'í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'
        }

# ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
def main():
    # í—¤ë”
    st.markdown('<div class="main-header">ğŸ¯ ì‹¤ì „í˜• ì—…ë¬´ ì‹œë®¬ë ˆì´í„° for ì‹ ì…</div>', unsafe_allow_html=True)
    st.markdown("### ğŸ’¼ ì‹ ì… ì§ì›ì„ ìœ„í•œ ê³ ê° ì‘ëŒ€ ì—°ìŠµ ë„êµ¬")
    
    # ì‚¬ì´ë“œë°” - ì„¤ì • ë° ë¬¸ì„œ ì—…ë¡œë“œ
    with st.sidebar:
        st.header("ğŸ“š ì—…ë¬´ ë§¤ë‰´ì–¼ ì—…ë¡œë“œ")
        
        uploaded_files = st.file_uploader(
            "ë§¤ë‰´ì–¼ íŒŒì¼ë“¤ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
            type=['pdf', 'txt', 'xlsx', 'xls'],
            accept_multiple_files=True,
            help="PDF, TXT, Excel íŒŒì¼ì„ ì§€ì›í•©ë‹ˆë‹¤"
        )
        
        st.header("âš™ï¸ AI ì„¤ì •")
        
        # Ollama ì—°ê²° í™•ì¸
        if 'ollama_connected' not in st.session_state:
            with st.spinner("ğŸ” AI ì‹œìŠ¤í…œ ì—°ê²° í™•ì¸ ì¤‘..."):
                connected, result = check_ollama_connection()
                st.session_state['ollama_connected'] = connected
        
        if st.session_state['ollama_connected']:
            st.success("âœ… AI ì‹œìŠ¤í…œ ì—°ê²°ë¨")
        else:
            st.error("âŒ AI ì‹œìŠ¤í…œ ì—°ê²° ì‹¤íŒ¨")
        
        model_name = st.selectbox(
            "AI ëª¨ë¸ ì„ íƒ",
            ["exaone3.5:2.4b-jetson", "llama3.2", "gemma2"],
            index=0
        )
        
        st.header("ğŸ“Š í•™ìŠµ í†µê³„")
        
        # ì„¸ì…˜ í†µê³„ ì´ˆê¸°í™”
        if 'stats' not in st.session_state:
            st.session_state.stats = {
                'total_simulations': 0,
                'customer_role_count': 0,
                'employee_role_count': 0,
                'avg_score': 0,
                'total_score': 0
            }
        
        stats = st.session_state.stats
        
        st.markdown(f"""
        <div class="stats-card">
            <h4>ì´ ì‹œë®¬ë ˆì´ì…˜: {stats['total_simulations']}</h4>
        </div>
        <div class="stats-card">
            <h4>ê³ ê° ì—­í• : {stats['customer_role_count']}</h4>
        </div>
        <div class="stats-card">
            <h4>ì§ì› ì—­í• : {stats['employee_role_count']}</h4>
        </div>
        """, unsafe_allow_html=True)
    
    # ë©”ì¸ ì»¨í…ì¸ 
    if uploaded_files:
        # ë¬¸ì„œ ì²˜ë¦¬
        if st.button("ğŸ“– ë§¤ë‰´ì–¼ í•™ìŠµ ì‹œì‘", type="primary"):
            with st.spinner("ğŸ“š ì—…ë¬´ ë§¤ë‰´ì–¼ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                all_text = ""
                for file in uploaded_files:
                    text = process_uploaded_file(file)
                    all_text += f"\n\n=== {file.name} ===\n{text}"
                
                if all_text:
                    st.success(f"âœ… {len(uploaded_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")
                    
                    # ì§€ì‹ ë² ì´ìŠ¤ ìƒì„±
                    embedding_model = init_embedding_model()
                    chunks = chunk_text(all_text)
                    collection = create_knowledge_base(chunks, embedding_model)
                    
                    st.session_state['knowledge_base'] = collection
                    st.session_state['embedding_model'] = embedding_model
                    st.session_state['manual_content'] = all_text
                    
                    st.info(f"ğŸ“– ì´ {len(chunks)}ê°œ í•™ìŠµ ë‹¨ìœ„ë¡œ ë¶„í•  ì™„ë£Œ")
        
        # ì‹œë®¬ë ˆì´ì…˜ ì„¹ì…˜
        if 'knowledge_base' in st.session_state:
            st.markdown("---")
            
            # ì—­í•  ì„ íƒ
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("""
                <div class="simulation-box">
                    <h3>ğŸ‘¤ ê³ ê° ì—­í•  ì—°ìŠµ</h3>
                    <p>AIê°€ ì§ì›ì´ ë˜ì–´ ê³ ê°ì¸ ë‹¹ì‹ ì˜ ë¬¸ì˜ì— ì‘ë‹µí•©ë‹ˆë‹¤.</p>
                </div>
                """, unsafe_allow_html=True)
                
                if st.button("ê³ ê°ìœ¼ë¡œ ì—°ìŠµí•˜ê¸°", key="customer_practice"):
                    st.session_state.current_role = "customer"
                    st.session_state.simulation_active = True
                    st.session_state.conversation_history = []
                    st.session_state.stats['customer_role_count'] += 1
                    st.rerun()
            
            with col2:
                st.markdown("""
                <div class="simulation-box">
                    <h3>ğŸ‘” ì§ì› ì—­í•  ì—°ìŠµ</h3>
                    <p>AIê°€ ë‹¤ì–‘í•œ ê³ ê°ì´ ë˜ì–´ ë‹¹ì‹ ì´ ì‘ëŒ€í•´ì•¼ í•  ìƒí™©ì„ ë§Œë“­ë‹ˆë‹¤.</p>
                </div>
                """, unsafe_allow_html=True)
                
                if st.button("ì§ì›ìœ¼ë¡œ ì—°ìŠµí•˜ê¸°", key="employee_practice"):
                    st.session_state.current_role = "employee"
                    st.session_state.simulation_active = True
                    st.session_state.conversation_history = []
                    st.session_state.stats['employee_role_count'] += 1
                    
                    # ê³ ê° ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
                    context = search_knowledge_base(
                        "ê³ ê° ë¬¸ì˜", 
                        st.session_state['knowledge_base'],
                        st.session_state['embedding_model']
                    )
                    scenario = generate_customer_scenario(" ".join(context), model_name)
                    st.session_state.customer_scenario = scenario
                    
                    st.rerun()
            
            # ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
            if hasattr(st.session_state, 'simulation_active') and st.session_state.simulation_active:
                st.markdown("---")
                
                # í˜„ì¬ ì—­í•  í‘œì‹œ
                if st.session_state.current_role == "customer":
                    st.markdown('<div class="role-badge customer-badge">ğŸ‘¤ ë‹¹ì‹ ì˜ ì—­í• : ê³ ê°</div>', unsafe_allow_html=True)
                    st.markdown("**ğŸ’¡ ìƒí™©:** AI ì§ì›ì—ê²Œ ë¬¸ì˜ì‚¬í•­ì„ ë§í•´ë³´ì„¸ìš”.")
                else:
                    st.markdown('<div class="role-badge employee-badge">ğŸ‘” ë‹¹ì‹ ì˜ ì—­í• : ì§ì›</div>', unsafe_allow_html=True)
                    
                    # ê³ ê° ì‹œë‚˜ë¦¬ì˜¤ í‘œì‹œ
                    scenario = st.session_state.get('customer_scenario', {})
                    if scenario:
                        st.markdown(f"""
                        **ğŸ“‹ ìƒí™©:** {scenario.get('situation', '')}  
                        **ğŸ‘¥ ê³ ê° ìœ í˜•:** {scenario.get('customer_type', '')}  
                        **ğŸ’¬ ê³ ê° ì²« ë§:** "{scenario.get('first_message', '')}"
                        """)
                
                # ëŒ€í™” íˆìŠ¤í† ë¦¬
                if 'conversation_history' not in st.session_state:
                    st.session_state.conversation_history = []
                
                # ì§ì› ëª¨ë“œì¼ ë•Œ ì²« ê³ ê° ë©”ì‹œì§€ ì¶”ê°€
                if (st.session_state.current_role == "employee" and 
                    not st.session_state.conversation_history and 
                    'customer_scenario' in st.session_state):
                    
                    first_msg = st.session_state.customer_scenario.get('first_message', '')
                    if first_msg:
                        st.session_state.conversation_history.append({
                            'role': 'customer_ai',
                            'message': first_msg
                        })
                
                # ëŒ€í™” í‘œì‹œ
                for msg in st.session_state.conversation_history:
                    if msg['role'] == 'user':
                        if st.session_state.current_role == "customer":
                            with st.chat_message("user"):
                                st.markdown(f"**ê³ ê° (ë‹¹ì‹ ):** {msg['message']}")
                        else:
                            with st.chat_message("user"):
                                st.markdown(f"**ì§ì› (ë‹¹ì‹ ):** {msg['message']}")
                    
                    elif msg['role'] == 'employee_ai':
                        with st.chat_message("assistant"):
                            st.markdown(f"**AI ì§ì›:** {msg['message']}")
                    
                    elif msg['role'] == 'customer_ai':
                        with st.chat_message("assistant"):
                            st.markdown(f"**AI ê³ ê°:** {msg['message']}")
                
                # ì‚¬ìš©ì ì…ë ¥
                if st.session_state.current_role == "customer":
                    user_input = st.chat_input("ê³ ê°ìœ¼ë¡œì„œ ë¬¸ì˜ì‚¬í•­ì„ ì…ë ¥í•˜ì„¸ìš”...")
                else:
                    user_input = st.chat_input("ì§ì›ìœ¼ë¡œì„œ ì‘ë‹µì„ ì…ë ¥í•˜ì„¸ìš”...")
                
                if user_input:
                    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ (ê³ ê°/ì§ì› ê³µí†µ)
                    st.session_state.conversation_history.append({
                        'role': 'user',
                        'message': user_input
                    })
                
                    # ë§¤ë‰´ì–¼ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
                    context = search_knowledge_base(
                        user_input,
                        st.session_state['knowledge_base'],
                        st.session_state['embedding_model']
                    )
                    context_text = " ".join(context)
                
                    if st.session_state.current_role == "customer":
                        # ğŸ‘¤ ê³ ê° ì—­í• : AIê°€ ì§ì›ìœ¼ë¡œ ì‘ë‹µ
                        ai_response = employee_ai_response(user_input, context_text, model_name)
                        st.session_state.conversation_history.append({
                            'role': 'employee_ai',
                            'message': ai_response
                        })
                
                    else:
                        # ğŸ‘” ì§ì› ì—­í• : ë‚´ê°€ ë‹µë³€ â†’ í‰ê°€ + ë‹¤ìŒ ê³ ê° ì§ˆë¬¸ ìë™ ìƒì„±
                
                        # 1) ë‚´ ë‹µë³€ í‰ê°€
                        evaluation = evaluate_response(user_input, context_text, model_name)
                        st.session_state.last_evaluation = evaluation
                
                        # 2) í†µê³„ ì—…ë°ì´íŠ¸
                        stats = st.session_state.stats
                        stats['total_score'] += evaluation['score']
                        stats['total_simulations'] += 1
                        stats['avg_score'] = (
                            stats['total_score'] / stats['total_simulations']
                            if stats['total_simulations'] > 0 else 0
                        )
                
                        # 3) ë‹¤ìŒ ê³ ê° ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
                        kb = st.session_state['knowledge_base']
                        emb_model = st.session_state['embedding_model']
                        next_ctx = search_knowledge_base("ê³ ê° ë¬¸ì˜", kb, emb_model)
                        next_scenario = generate_customer_scenario(" ".join(next_ctx), model_name)
                        st.session_state.customer_scenario = next_scenario
                
                        # 4) ìƒˆ ê³ ê°ì˜ "ì²« ë§"ì„ ë°”ë¡œ ì±„íŒ…ì°½ì— ì¶”ê°€
                        next_first = next_scenario.get('first_message', '')
                        if next_first:
                            st.session_state.conversation_history.append({
                                'role': 'customer_ai',
                                'message': next_first
                            })
                
                    st.rerun()

                
                # í‰ê°€ ê²°ê³¼ í‘œì‹œ (ì§ì› ëª¨ë“œ)
                if (st.session_state.current_role == "employee" and 
                    hasattr(st.session_state, 'last_evaluation')):
                    
                    eval_data = st.session_state.last_evaluation
                    
                    st.markdown("### ğŸ“Š ì‘ë‹µ í‰ê°€")
                    
                    col_eval1, col_eval2 = st.columns([1, 2])
                    
                    with col_eval1:
                        score_percentage = (eval_data['score'] / eval_data['max_score']) * 100
                        st.metric("ì ìˆ˜", f"{eval_data['score']}/{eval_data['max_score']}", f"{score_percentage:.0f}%")
                    
                    with col_eval2:
                        st.text_area("ìƒì„¸ í”¼ë“œë°±", eval_data['feedback'], height=100, disabled=True)
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    st.session_state.stats['total_score'] += eval_data['score']
                    st.session_state.stats['total_simulations'] += 1
                    if st.session_state.stats['total_simulations'] > 0:
                        st.session_state.stats['avg_score'] = st.session_state.stats['total_score'] / st.session_state.stats['total_simulations']
                
                # ì‹œë®¬ë ˆì´ì…˜ ì¢…ë£Œ ë²„íŠ¼
                col_end1, col_end2 = st.columns([1, 1])
                with col_end1:
                    if st.button("ğŸ”„ ìƒˆ ì‹œë‚˜ë¦¬ì˜¤ ì‹œì‘"):
                        st.session_state.conversation_history = []
                        if st.session_state.current_role == "employee":
                            # ìƒˆë¡œìš´ ê³ ê° ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
                            context = search_knowledge_base(
                                "ê³ ê° ë¬¸ì˜",
                                st.session_state['knowledge_base'],
                                st.session_state['embedding_model']
                            )
                            scenario = generate_customer_scenario(" ".join(context), model_name)
                            st.session_state.customer_scenario = scenario
                        st.rerun()
                
                with col_end2:
                    if st.button("âŒ ì‹œë®¬ë ˆì´ì…˜ ì¢…ë£Œ"):
                        st.session_state.simulation_active = False
                        st.rerun()
    
    else:
        # ì†Œê°œ ì„¹ì…˜
        st.markdown("""
        ## ğŸš€ ì‹œì‘í•˜ê¸°
        
        ### 1ë‹¨ê³„: ì—…ë¬´ ë§¤ë‰´ì–¼ ì—…ë¡œë“œ
        - ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ì—…ë¬´ ë§¤ë‰´ì–¼ íŒŒì¼ë“¤ì„ ì—…ë¡œë“œí•˜ì„¸ìš”
        - **ì§€ì› í˜•ì‹:** PDF, TXT, Excel íŒŒì¼
        - **ì˜ˆì‹œ:** ê³ ê°ì‘ëŒ€ ë§¤ë‰´ì–¼, FAQ, ì„œë¹„ìŠ¤ ì•ˆë‚´ì„œ ë“±
        
        ### 2ë‹¨ê³„: ì—­í•  ì„ íƒ
        - **ğŸ‘¤ ê³ ê° ì—­í• :** AI ì§ì›ê³¼ ëŒ€í™”í•˜ë©° ê³ ê° ì…ì¥ ì²´í—˜
        - **ğŸ‘” ì§ì› ì—­í• :** AI ê³ ê°ì˜ ë¬¸ì˜ì— ì‘ëŒ€í•˜ë©° ì‹¤ì „ ì—°ìŠµ
        
        ### 3ë‹¨ê³„: ì‹¤ì „ ì—°ìŠµ
        - ì‹¤ì œ ì—…ë¬´ì™€ ìœ ì‚¬í•œ ìƒí™©ì—ì„œ ì—°ìŠµ
        - AIì˜ ì‹¤ì‹œê°„ í”¼ë“œë°±ìœ¼ë¡œ ê°œì„ ì  íŒŒì•…
        - ë°˜ë³µ í•™ìŠµìœ¼ë¡œ ìì‹ ê° í–¥ìƒ
        
        ---
        
        ## ğŸ’¡ ì£¼ìš” ê¸°ëŠ¥
        
        - âœ… **ë‹¤ì–‘í•œ ë¬¸ì„œ ì§€ì›**: PDF, TXT, Excel íŒŒì¼ ì—…ë¡œë“œ
        - âœ… **ì–‘ë°©í–¥ ì‹œë®¬ë ˆì´ì…˜**: ê³ ê°â†”ì§ì› ì—­í•  ì „í™˜
        - âœ… **ì‹¤ì‹œê°„ í‰ê°€**: AIì˜ ìƒì„¸í•œ í”¼ë“œë°± ì œê³µ
        - âœ… **í•™ìŠµ í†µê³„**: ì—°ìŠµ ì§„í–‰ë„ ë° ì„±ê³¼ ì¶”ì 
        - âœ… **ì‹¤ì „ ì‹œë‚˜ë¦¬ì˜¤**: ë§¤ë‰´ì–¼ ê¸°ë°˜ ë‹¤ì–‘í•œ ìƒí™© ìƒì„±
        
        **ğŸ¯ ì§€ê¸ˆ ë°”ë¡œ ë§¤ë‰´ì–¼ì„ ì—…ë¡œë“œí•˜ê³  ì—°ìŠµì„ ì‹œì‘í•´ë³´ì„¸ìš”!**
        """)

if __name__ == "__main__":
    main()


